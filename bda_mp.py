# -*- coding: utf-8 -*-
"""BDA mp.ipynb

Automatically generated by Colaboratory.

# **Importing Libraries**
"""

!pip install seaborn #seaborn is used for pairplot

#plotly used for plotting Interactive Graphs
!pip install plotly
import plotly.offline as py
import plotly.express as px
import plotly.graph_objs as go

!pip install git+https://github.com/tensorflow/docs # Use some functions from tensorflow_docs

import pathlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

print(tf.__version__)

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

"""# **Get Data**"""

dataset_path = "https://docs.google.com/uc?export=download&id=15GuwyGYyq1mjrYyPVV-j3B9d_Shm3_SI" # for direct download from drive- https://docs.google.com/uc?export=download&id=
raw_dataset = pd.read_csv(dataset_path)
raw_dataset

dataset = raw_dataset.copy()
dataset

print(dataset.shape) #(rows,columns)

dataset.shape[0] # shape[0] gives the number of rows in the dataframe

dataset.shape[1] # shape[1] gives the number of columns in the dataframe

dataset.describe()

dataset.info()

"""## **Pre Process, Clean the Data** 

*   Store - a unique Id for each store
*   Sales - the turnover for any given day (this is what you are predicting)
*   Customers - the number of customers on a given day
*   Open - an indicator for whether the store was open: 0 = closed, 1 = open
*   StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None
*   SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools
*   Promo - indicates whether a store is running a promo on that day
*   List item

# **Eliminate Null if exists**
"""

dataset.isnull()

dataset.isnull().sum()

"""# Date Transformation

# Convert Date **dtype** from `object` to `datetime`
"""

dataset['Date'] = pd.to_datetime(dataset['Date'], utc=False)
dataset.info()

"""## Converting Date into Separate Year, Month and Day Column"""

import datetime
dataset['Year'] = pd.DatetimeIndex(dataset['Date']).year
dataset['Month'] = pd.DatetimeIndex(dataset['Date']).month
dataset['Day'] = pd.DatetimeIndex(dataset['Date']).day
dataset.head(10)

"""# Merging same string and int values"""

dataset.StateHoliday.unique()

# StateHoliday column has values 0 & "0", So, we need to merge values with 0 to "0"
dataset["StateHoliday"].loc[dataset["StateHoliday"] == 0] = "0"
dataset

dataset.StateHoliday.unique()

"""# Reorder"""

dataset = dataset[['Store', 'DayOfWeek', 'Date', 'Year', 'Month', 'Day', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'Customers', 'Sales']]
dataset

"""# Analyzing Data"""

from google.colab import files
plt.figure(figsize=(12,8))
sns.heatmap(pd.pivot_table(dataset,'Sales','Month','Year'),annot=True,fmt='.2f',cmap='BuGn_r')

plt.savefig('test.png')

files.download('test.png')

px.box(x=dataset['Month'],y=dataset['Customers'],color=dataset['Year'])

#sns.pairplot(dataset[['Store', 'DayOfWeek', 'Date', 'Year', 'Month', 'Day', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'Customers', 'Sales']], diag_kind="kde")

sns.pairplot(dataset, x_vars='Customers',  y_vars='Sales', height=5, aspect=1, kind='scatter')
plt.show()

sns.pairplot(dataset, x_vars='Customers', y_vars='Sales', height=5, aspect=1, kind='reg')
plt.show()

sns.barplot(data=dataset, x='StateHoliday', y='Sales')
plt.show()

mask = (dataset['StateHoliday'] != '0') & (dataset['Sales'] > 0)
sns.barplot(x='StateHoliday', y='Sales', data=dataset[mask]) # a = public holiday, b = Easter holiday, c = Christmas, 0 = None

sns.barplot(data=dataset, x='Promo', y='Sales')
plt.show()

sns.barplot(data=dataset, x='Promo', y='Customers')
plt.show()

#plt.figure(figsize=(10,250))
#sns.barplot(data=dataset, x='Sales', y='Store', orient='h')

#Co-relation
sns.set(style="white")

# Compute the correlation matrix
corr = dataset.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

# Correlation Matrix Heatmap
f, ax = plt.subplots(figsize=(10, 6))
corr = dataset.corr()
hm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap="coolwarm",fmt='.2f',
                 linewidths=.05)
f.subplots_adjust(top=0.93)
t= f.suptitle('Correlation Heatmap', fontsize=14)

from mpl_toolkits.mplot3d import Axes3D
Customers = dataset['Customers'].values
Sales = dataset['Sales'].values
Open = dataset['Open'].values

# Ploting the scores as scatter plot
fig = plt.figure()
ax = Axes3D(fig)
ax.scatter( Sales, Open, Customers, color='#ef1234')
plt.show()

"""# Converting Categorical Data into Numerical

StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays.

StateHoliday is a Categorical Column with following categories:

`a = public holiday, b = Easter holiday, c = Christmas, 0 = None`
"""

dataset

# One-Hot-Encoding
data = pd.get_dummies(dataset, prefix=['StateHoliday'], columns=['StateHoliday'])
data

# To avoid multi-collinearity, we have to drop one of the dummy columns.
data.drop('StateHoliday_0', axis=1, inplace=True)
data

"""# **Data Wrangling**
Data Wrangling is a technique that is executed at the time of making an interactive model. In other words, it is used to convert the raw data into the format that is convenient for the consumption of data.
"""

data.drop(['Date'], axis = 1, inplace = True)
data

df = data.copy()
df.drop(['Store', 'DayOfWeek', 'Year', 'Month', 'Day'], axis=1, inplace=True)
df.head()

df.drop(['StateHoliday_a',	'StateHoliday_b', 'StateHoliday_c'], axis=1, inplace=True)
df

#Co-relation
sns.set(style="white")

# Compute the correlation matrix
corr = df.corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})

"""## **Split the data into train and test** """

train_dataset = df.sample(frac=0.8,random_state=0) #train = 80%,  random_state = any int value means every time when you run your program you will get the same output for train and test dataset, random_state is None by default which means every time when you run your program you will get different output because of splitting between train and test varies within 
test_dataset = df.drop(train_dataset.index) #remove train_dataset from df to get test_dataset
train_dataset

"""# Split features from labels
Separate the target value, or "label", from the features.

**This label is the value that you will train the model to predict.**  *(Here: Sales)*
"""

train_labels = train_dataset.pop('Sales') #train_labels is Output(target)
test_labels = test_dataset.pop('Sales')

train_labels

train_dataset

train_stats = train_dataset.describe()
train_stats = train_stats.transpose()
train_stats

"""# **Normalize the Data(features)**
It is good practice to normalize features that use different scales and ranges. Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.

Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on.
"""

def norm(x):
  return (x - train_stats['mean']) / train_stats['std'] # Z-score Normalization where STD is Standard Deviation
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)

normed_train_data

normed_test_data

"""# Models

# Linear Regression Model

It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.

 $Y = a + bX$  
 where $Y$ – Dependent Variable,  $a$ – intercept, $X$ – Independent variable,  $b$ – Slope

Multiple linear regression is used to estimate the relationship between two or more independent(features) variables and one dependent(label) variable.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
 
#Define a Multiple Linear Regression Model
# Model Intialization
linearRegressor = LinearRegression()
 
# X and y Values
X = normed_train_data
y = train_labels
 
 
# Data Fitting
linearRegressor = linearRegressor.fit(X, y)
 
# Y Prediction
y_pred = linearRegressor.predict(X)
 
# Model Evaluation
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = linearRegressor.score(X, y) #score(Input,output) method uses r2_score(y_true, y_pred) method from sklearn.metrics
 
print(rmse)
#print(r2)
r2_score(y, y_pred)

#predicting Test Data
y_test_pred = linearRegressor.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

#Plot Actual vs Predicted Sales
actual_chart = go.Scatter(x=test_labels.index, y=test_labels, name= 'Actual Sales')
predict_chart = go.Scatter(x=test_labels.index, y=y_test_pred, name= 'Predicted Sales')
py.iplot([predict_chart, actual_chart])

"""## XGBoost

XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.
"""

import xgboost as xgb

# X and y Values
X = normed_train_data
y = train_labels
X.head()

from sklearn.metrics import mean_squared_error, r2_score

# Model Intialization
model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state =7, nthread = -1)

# Data Fitting
model_xgb.fit(X, y, verbose=True)

# y Prediction (Predicting Sales for Train Data)
y_pred = model_xgb.predict(X)

# Model Evaluation
rmse = np.sqrt(mean_squared_error(y, y_pred))
#print(rmse)
r2_score(y, y_pred)

#predicting Test Data
y_test_pred = model_xgb.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

#Plot Actual vs Predicted Sales
import plotly.graph_objs as go
actual_chart = go.Scatter(x=test_labels.index, y=test_labels, name= 'Actual Sales')
predict_chart = go.Scatter(x=test_labels.index, y=y_test_pred, name= 'Predicted Sales')
py.iplot([predict_chart, actual_chart])

"""## Random Forest Regressor

Random forest is collection of tress(forest) and it builds multiple decision trees and merges them together to get a more accurate and stable prediction.It can be used for both ***classification*** and ***regression problems***.
"""

# Fitting Random Forest Regression to the dataset 
# import the random forest regressor 
from sklearn.ensemble import RandomForestRegressor 

X = normed_train_data
y = train_labels

# create forest regressor object 
forestregressor = RandomForestRegressor(n_estimators = 100, random_state = 0, verbose=1) 
  
# fit the forest regressor with x and y data 
forestregressor.fit(X, y)

# Y Prediction
y_pred = forestregressor.predict(X)

# Model Evaluation
rmse = np.sqrt(mean_squared_error(y, y_pred))
r2 = forestregressor.score(X, y)
print(rmse)
#print(r2)
r2_score(y, y_pred)

#Testing Test Data
y_test_pred = forestregressor.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

#Plot Actual vs Predicted Sales
import plotly.graph_objs as go
actual_chart = go.Scatter(x=test_labels.index, y=test_labels, name= 'Actual Sales')
predict_chart = go.Scatter(x=test_labels.index, y=y_test_pred, name= 'Predicted Sales')
py.iplot([predict_chart, actual_chart])

"""## Catboost

CatBoost is an open-source gradient boosting on decision trees library developed by Yandex. CatBoost can be used for solving problems, such as: Classification (binary, multi-class), Regression, Ranking, etc

CatBoostRegressor is used for Regression problems
"""

!pip install catboost
from catboost import CatBoostRegressor

X = normed_train_data
y = train_labels

cb_model = CatBoostRegressor(iterations=5000,
                             learning_rate=0.05,
                             depth=10,
                             random_seed = 42,
                             bagging_temperature = 0.2,
                             od_type='Iter',
                             metric_period = 500,
                             od_wait=20)
cb_model.fit(X, y) #Train the model

# Model Evaluation
r2_score(y, cb_model.predict(X))

#Testing Test Data
y_test_pred = cb_model.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

#Plot Actual vs Predicted Sales
import plotly.graph_objs as go
actual_chart = go.Scatter(x=test_labels.index, y=test_labels, name= 'Actual Sales')
predict_chart = go.Scatter(x=test_labels.index, y=y_test_pred, name= 'Predicted Sales')
py.iplot([predict_chart, actual_chart])

"""## Light GBM(Light Gradient Boosting Machine)

LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:

* Faster training speed and higher efficiency.
* Lower memory usage.
* Better accuracy.
* Support of parallel and GPU learning.
* Capable of handling large-scale data.
"""

import lightgbm as lgb
from sklearn.model_selection import KFold, GridSearchCV

X = normed_train_data
y = train_labels

kfold = KFold(n_splits=5, random_state = 2020, shuffle = True)

model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)
model_lgb.fit(X, y)

# Model Evaluation
y_pred = model_lgb.predict(X)
r2_score(y, y_pred)

#Testing Test Data
y_test_pred = model_lgb.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

"""##Stochastic Gradient Descent(SGD)

Stochastic means random , so in Stochastic Gradient Descent dataset sample is choosedn random instead of the whole dataset.hough, using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, but the problem arises when our datasets get really huge and for that SGD come in action
"""

from sklearn.linear_model import SGDRegressor

X = normed_train_data
y = train_labels

SGD = SGDRegressor(max_iter = 10000)
SGD.fit(X, y) #Train the model

# Model Evaluation
y_pred = SGD.predict(X)
r2_score(y, y_pred)

#Testing Test Data
y_test_pred = SGD.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

"""## LASSO (least absolute shrinkage and selection operator)

In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Though originally defined for least squares, lasso regularization is easily extended to a wide variety of statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, in a straightforward fashion
"""

from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler

X = normed_train_data
y = train_labels

lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))
lasso.fit(X, y)

# Model Evaluation
r2_score(y, lasso.predict(X))

#Testing Test Data
y_test_pred = lasso.predict(normed_test_data)
r2_score(test_labels, y_test_pred)

"""## LSTM(Long short-term memory) Recurrent Neural Network Model

Long short-term memory (LSTM) is an Artificial Recurrent Neural Network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections.

LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series.


The input to every **LSTM layer** must be three-dimensional.

The three dimensions of this input are:

* **Samples**. One sequence is one sample. A batch is comprised of one or more samples.
* **Time Steps** . One time step is one point of observation in the sample.
* **Features**. One feature is one observation at a time step.

### Build the Model
"""

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM

# TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).
optimizer = keras.optimizers.RMSprop(0.001) #RMSProp as optimizer with Learning Rate of 0.003
#Minimum Squared Loss(mse) is Loss function (Standard function for Regression)
model_lstm = Sequential()
model_lstm.add(LSTM(128, input_shape=(1,4)))
model_lstm.add(Dense(1))
model_lstm.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])

# Reshape the data between -1 and 1 and to 3D
from sklearn.preprocessing import StandardScaler,MinMaxScaler
scaler = StandardScaler()
scaler = MinMaxScaler(feature_range=(-1, 1))
x_train_scaled = scaler.fit_transform(train_dataset)
#x_valid_scaled = scaler.fit_transform(x_val)

x_train_reshaped = x_train_scaled.reshape((x_train_scaled.shape[0], 1, x_train_scaled.shape[1]))

x_train_reshaped

train_labels

model_lstm.summary()

"""### Train the model"""

history = model_lstm.fit(x_train_reshaped, train_labels, validation_split = 0.3, epochs=50, batch_size=128, verbose=1, shuffle=False)

hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()

plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)

plotter.plot({'Basic': history}, metric = "mse")
plt.ylim([0, 10000000])
plt.ylabel('MSE [Sales]')

plotter.plot({'Basic': history}, metric = "mae")
plt.ylim([0, 6000])
plt.ylabel('MAE [Sales]')

"""### Save Model and Retrain Model"""

#Install PyDrive library into Google Colab notebook
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

#Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()                       
drive = GoogleDrive(gauth)

!pip install pyyaml h5py  # Required to save models in HDF5 format

# Save Model or weights on google drive and create on Colab directory in Google Drive

# Save the entire model to a HDF5 file.
# The '.h5' extension indicates that the model should be saved to HDF5.
model_lstm.save('model_mae_925_mse_1982202.h5')
model_file = drive.CreateFile({'title' : 'model_mae_925_mse_1982202.h5'}) # model_mae_   _mse_   .h5 is the name of the model uploaded to Drive                   
model_file.SetContentFile('model_mae_925_mse_1982202.h5')                       
model_file.Upload()

# download to google drive                       
drive.CreateFile({'id': model_file.get('id')}) ### very important to run this last for loading the model in new notebook

#Load model from google drive into colab
file_obj = drive.CreateFile({'id': '1wXwLZEb2c4uBt7oVfVg8FYLHL2Tqf1lJ'})                       
file_obj.GetContentFile('new_model.h5') #new_model.h5 is the name of model uploaded to colab

ls

# Recreate the exact same model, including its weights and the optimizer
model_lstm = tf.keras.models.load_model('new_model.h5') #very important

# Show the model architecture
model_lstm.summary()

#initialising new model with new optimizer, learning rate and compiling
optimizer = keras.optimizers.RMSprop(0.01) #RMSProp as optimizer with Learning Rate of 0.001
model_lstm.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])

#Continue Training New Model
history = model_lstm.fit(x_train_reshaped, train_labels, validation_split = 0.2, epochs=50, batch_size=128, verbose=1, shuffle=False)

"""### Make Predictions

Finally, predict Sales values using data in the testing set:
"""

x_test_scaled = scaler.fit_transform(test_dataset)
x_test_reshaped = x_test_scaled.reshape((x_test_scaled.shape[0], 1, x_test_scaled.shape[1]))
x_test_reshaped

test_predictions = model_lstm.predict(x_test_reshaped)

test_predictions

y_test_pred = test_predictions.astype(int) 
y_test_pred

y_test_pred.ndim # y_test_pred is currently a 2D array

y_test_pred = y_test_pred.flatten() #converting into 1D array
y_test_pred

test_labels

actual_values = test_labels.values
actual_values

y_test_pred = pd.Series(y_test_pred) #creating panda dataframe with index for plotting
y_test_pred

actual_values = pd.Series(actual_values)
actual_values

#Plot Actual vs Predicted Sales
actual_chart = go.Scatter(x=actual_values.index, y=actual_values, name= 'Actual Sales')
predict_chart = go.Scatter(x=y_test_pred.index, y=y_test_pred, name= 'Predicted Sales')
py.iplot([predict_chart, actual_chart])

"""# TIME SERIES FORECASTING
Making predictions about the future is called extrapolation in the classical statistical handling of time series data.

More modern fields focus on the topic and refer to it as time series forecasting.

Forecasting involves taking models fit on historical data and using them to predict future observations.

Descriptive models can borrow for the future (i.e. to smooth or remove noise), they only seek to best describe the data.

An important distinction in forecasting is that the future is completely unavailable and must only be estimated from what has already happened.

## Prophet

Prophet is an extremely easy tool for analysts to produce reliable forecasts

1. Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. So first, let’s convert the dataframe to the appropriate format.
2. Create an instance of the Prophet class and then fit our dataframe to it.
3. Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). Then specify the number of days to forecast using the periods parameter.
4. Call predict to make a prediction and store it in the forecast dataframe. What’s neat here is that you can inspect the dataframe and see the predictions as well as the lower and upper boundaries of the uncertainty interval.
"""

!pip install plotly

!pip install fbprophet

import plotly.offline as py
import plotly.express as px
from fbprophet import Prophet
from fbprophet.plot import plot_plotly, add_changepoints_to_plot

dataset

"""### Overall Sales Prediction for Every Stores """

pr_data = dataset[['Date', 'Sales']]
pr_data.head()

#Grouping Data for every Store each Day
pr_data = pr_data.groupby('Date', as_index=False).agg({"Sales": "sum"})  # as_index : For aggregated output, return object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively “SQL-style” grouped output
pr_data

pr_data.columns = ['ds','y'] #Dataframe must have columns "ds" and "y" with the dates and values respectively
pr_data

pr_data.info()

# Model and Forecast
m=Prophet(daily_seasonality=True)
m.fit(pr_data)
future=m.make_future_dataframe(periods=600) # periods - Number of days for which Sales has to be Predicted in Future
forecast=m.predict(future)
forecast

fig = plot_plotly(m, forecast)
py.iplot(fig) 

fig = m.plot(forecast,xlabel='Date',ylabel='Sales')

fig = m.plot_components(forecast)

"""### Sales Prediction for a Particular Store"""

dataset

dataset.Store.unique()

storeid = input('Enter Store ID(Enter a number from 1 to 1115) for its Sales Prediction: ')

store_data = dataset[dataset['Store'] == int(storeid)]
store_data

pr_store_data = store_data[['Date', 'Sales']]
pr_store_data.head()

pr_store_data = pr_store_data.groupby('Date', as_index=False).agg({"Sales": "sum"})
pr_store_data.columns=['ds','y']
pr_store_data

# Model and Forecast
m=Prophet(daily_seasonality=True )
m.fit(pr_store_data)
future=m.make_future_dataframe(periods= 365 ) #periods - Number of days for which Sales has to be Predicted in Future
forecast=m.predict(future)
forecast

fig = plot_plotly(m, forecast)
py.iplot(fig) 

fig = m.plot(forecast,xlabel='Date',ylabel='StoreSales')

fig = m.plot_components(forecast)

"""# **THANK YOU**"""

